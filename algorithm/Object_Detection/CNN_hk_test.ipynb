{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os \n",
    "from keras.models import Sequential \n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hk_label_conversion(label) :\n",
    "  # Joseph Lin 2/18/2023\n",
    "  # This function converts an integer label of the image to the string equivalent classes \n",
    "  # Function input: \n",
    "  # label: an integer number specifying the classes of the image \n",
    "  # Function output: \n",
    "  # returns a string equivalent of the label of the image\n",
    "  if label == 0 : \n",
    "    return \"10 dollars\"\n",
    "  elif label == 1: \n",
    "    return \"20 dollars\"\n",
    "  elif label == 2: \n",
    "    return \"50 dollars\" \n",
    "  elif label == 3: \n",
    "    return \"100 dollars\" \n",
    "  elif label == 4: \n",
    "    return \"500 dollars\" \n",
    "  elif label == 5: \n",
    "    return \"1000 dollars\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hk_data(directory, getIndividual=None): \n",
    "  # Joseph Lin 2/18/2023\n",
    "  # This function loads the thai dataset from the image folder \n",
    "  # do the transformation to the image (resizing and normalizing) \n",
    "  # then split the image into training and testing dataset\n",
    "  # Function inputs: \n",
    "  # directory: the path of the folder that contains all the thai images\n",
    "  # getIndividual: a boolean specifiying which kind of data should be returned\n",
    "\n",
    "  # image and label for each classes of bank notes\n",
    "  img_10 = [] \n",
    "  lbl_10 = [] \n",
    "  img_20 = [] \n",
    "  lbl_20 = [] \n",
    "  img_50 = [] \n",
    "  lbl_50 = [] \n",
    "  img_100 = [] \n",
    "  lbl_100 = [] \n",
    "  img_500 = [] \n",
    "  lbl_500 = []\n",
    "  img_1000 = []\n",
    "  lbl_1000 = [] \n",
    "  # initialize the training size \n",
    "  training_size = 0.8\n",
    "\n",
    "  for image_class in os.listdir(directory) : \n",
    "    for image_name in os.listdir(os.path.join(directory, image_class)):\n",
    "      # open the image at the image file path\n",
    "      image = Image.open(os.path.join(directory, image_class, image_name))\n",
    "      # resize the image to 32x32\n",
    "      image.thumbnail((300,300)) \n",
    "      # normalize the image\n",
    "      image = np.array(image) / 255 \n",
    "      if image_class == \"10 dollars\": # if the image belongs to 10 hk dollars\n",
    "        lbel = 0\n",
    "        img_10.append(image) \n",
    "        lbl_10.append(lbel) \n",
    "      elif image_class == \"20 dollars\": # if the image belongs to 20 hk dollars \n",
    "        lbel = 1\n",
    "        img_20.append(image) \n",
    "        lbl_20.append(lbel) \n",
    "      elif image_class == \"50 dollars\": # if the image belongs to 50 hk dollars \n",
    "        lbel = 2\n",
    "        img_50.append(image)\n",
    "        lbl_50.append(lbel)       \n",
    "      elif image_class == \"100 dollars\": # if the image belongs to 100 hk dollars \n",
    "        lbel = 3\n",
    "        img_100.append(image) \n",
    "        lbl_100.append(lbel) \n",
    "      elif image_class == \"500 dollars\": # if the image belongs to 500 hk dollars \n",
    "        lbel = 4\n",
    "        img_500.append(image) \n",
    "        lbl_500.append(lbel) \n",
    "      elif image_class == \"1000 dollars\": # if the image belongs to 1000 hk dollars \n",
    "        lbel = 5\n",
    "        img_1000.append(image) \n",
    "        lbl_1000.append(lbel) \n",
    "\n",
    "  # equally split the dataset into train and test for each classes of images\n",
    "  hk10_img_train, hk10_img_test, hk10_label_train, hk10_label_test = train_test_split(img_10, lbl_10, train_size=training_size) \n",
    "  hk20_img_train, hk20_img_test, hk20_label_train, hk20_label_test = train_test_split(img_20, lbl_20, train_size=training_size) \n",
    "  hk50_img_train, hk50_img_test, hk50_label_train, hk50_label_test = train_test_split(img_50, lbl_50, train_size=training_size) \n",
    "  hk100_img_train, hk100_img_test, hk100_label_train, hk100_label_test = train_test_split(img_100, lbl_100, train_size=training_size)\n",
    "  hk500_img_train, hk500_img_test, hk500_label_train, hk500_label_test = train_test_split(img_500, lbl_500, train_size=training_size) \n",
    "  hk1000_img_train, hk1000_img_test, hk1000_label_train, hk1000_label_test = train_test_split(img_1000, lbl_1000, train_size=training_size) \n",
    " \n",
    "  \n",
    "  # combine the data together into train and test set\n",
    "  x_train = np.vstack([hk10_img_train, hk20_img_train, hk50_img_train, hk100_img_train, hk500_img_train, hk1000_img_train])\n",
    "  y_train = np.hstack([hk10_label_train, hk20_label_train, hk50_label_train, hk100_label_train, hk500_label_train, hk1000_label_train])\n",
    "  x_test = np.vstack([hk10_img_test, hk20_img_test, hk50_img_test, hk100_img_test, hk500_img_test, hk1000_img_test])\n",
    "  y_test = np.hstack([hk10_label_test, hk20_label_test, hk50_label_test, hk100_label_test, hk500_label_test, hk1000_label_test])\n",
    "\n",
    "  return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Thai training images = 1176\n",
      "number of Thai testing images = 294\n"
     ]
    }
   ],
   "source": [
    "# Leng Lohanakakul 11/7/2022\n",
    "# Main function for loading the combined and individual dataset\n",
    "\n",
    "data_dir = '/data/lin1223/Dataset'\n",
    "\n",
    "# load the dataset and split it into train and test set \n",
    "x_thai_train, y_thai_train, x_thai_test, y_thai_test = load_hk_data(data_dir, False)\n",
    "\n",
    "# number of training and testing dataset \n",
    "print(f\"number of Thai training images = {y_thai_train.shape[0]}\")\n",
    "print(f\"number of Thai testing images = {y_thai_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (None, 223, 298, 32)      896       \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 221, 296, 64)      18496     \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 55, 74, 64)        0         \n",
      "                                                                 \n",
      " flat (Flatten)              (None, 260480)            0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 7)                 1823367   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,842,759\n",
      "Trainable params: 1,842,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Leng Lohanakakul 11/7/2022\n",
    "# Define a convolutional neural network that takes an input shape of 128x128x3\n",
    "cnn = Sequential() \n",
    "cnn.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(225,300,3), name='conv1'))\n",
    "cnn.add(Conv2D(64, kernel_size=3, activation='relu', name='conv2'))\n",
    "cnn.add(MaxPool2D(pool_size=(4,4), name='pool2'))\n",
    "cnn.add(Flatten(name='flat'))\n",
    "cnn.add(Dense(7, activation='softmax', name='output'))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176,)\n",
      "(294,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(y_thai_test\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[0;32m---> 12\u001b[0m training \u001b[39m=\u001b[39m cnn\u001b[39m.\u001b[39;49mfit(x_thai_train, y_thai_train, epochs\u001b[39m=\u001b[39;49mn_epochs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "# Leng Lohanakakul 11/7/2022\n",
    "# Main function for compiling the model and defining the hyperparameters \n",
    "\n",
    "# define the optimizer and loss function used for the neural network\n",
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model with training dataset over n_epochs \n",
    "print(y_thai_train.shape)\n",
    "print(y_thai_test.shape)\n",
    "\n",
    "x_thai_train = np.asarray(x_thai_train).astype(np.float32)\n",
    "y_thai_train = np.asarray(y_thai_train).astype(np.float32)\n",
    "\n",
    "n_epochs = 15\n",
    "training = cnn.fit(x_thai_train, y_thai_train, epochs=n_epochs)\n",
    "\n",
    "#save model to a file \n",
    "# cnn.save(\"/content/drive/My Drive/VIP/model3_uae.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bce6ae08df7708670a76d2386bacb70029ac577a8fcc0e55380e271d0bfb832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
